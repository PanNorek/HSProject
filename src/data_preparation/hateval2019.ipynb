{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, GRU, Input, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('..\\..\\data\\hateval2019_en_test.csv')\n",
    "train_data = pd.read_csv('..\\..\\data\\hateval2019_en_train.csv')\n",
    "dev_data = pd.read_csv('..\\..\\data\\hateval2019_en_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5)\n",
      "(9000, 5)\n",
      "(1000, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data.shape), print(train_data.shape), print(dev_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13000 entries, 0 to 2999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      13000 non-null  int64 \n",
      " 1   text    13000 non-null  object\n",
      " 2   HS      13000 non-null  int64 \n",
      " 3   TR      13000 non-null  int64 \n",
      " 4   AG      13000 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 609.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13000 entries, 0 to 2999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      13000 non-null  int64 \n",
      " 1   text    13000 non-null  object\n",
      " 2   HS      13000 non-null  int64 \n",
      " 3   TR      13000 non-null  int64 \n",
      " 4   AG      13000 non-null  int64 \n",
      "dtypes: int64(4), object(1)\n",
      "memory usage: 609.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id     TR  AG\n",
       "201    0   0     1\n",
       "8871   1   1     1\n",
       "8861   0   0     1\n",
       "8862   1   1     1\n",
       "8863   1   1     1\n",
       "                ..\n",
       "4537   0   0     1\n",
       "4538   0   0     1\n",
       "4539   0   1     1\n",
       "4540   0   0     1\n",
       "34597  0   0     1\n",
       "Length: 13000, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['id','TR','AG']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['id','TR','AG'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_pred, y_train, y_pred = train_test_split(data['text'], data['HS'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Training Data'}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train.value_counts().plot(kind='bar',\n",
    " title='Training Data', \n",
    " color='#00bcd4',\n",
    " x='labels',\n",
    " y='count',\n",
    " rot=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    print(text)\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text = \" \".join([w for w in twtk.tokenize(text) if w != \"\" and w is not None])\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(tokens):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tokens if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "\n",
    "def removeURL(text):\n",
    "    newText = re.sub('http\\\\S+', '', text, flags=re.MULTILINE)\n",
    "    return str(newText)\n",
    "\n",
    "def removeNum(text):\n",
    "    newText = re.sub('\\\\d+', '', text)\n",
    "    return newText\n",
    "\n",
    "def removeHashtags(tokens):\n",
    "    toks = [ tok for tok in tokens if tok[0] != '#']\n",
    "#     if segment == True:\n",
    "#         segTool = Analyzer('en')\n",
    "#         for i, tag in enumerate(self.hashtags):\n",
    "#             text = tag.lstrip('#')\n",
    "#             segmented = segTool.segment(text)\n",
    "\n",
    "    return toks\n",
    "\n",
    "def stemTweet(tokens):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweet(tweet, remove_swords = True, remove_url = True, remove_hashtags = True, remove_num = True, stem_tweet = True):\n",
    "#     text = tweet.translate(string.punctuation)   -> to figure out what it does ?\n",
    "    \"\"\"\n",
    "        Tokenize the tweet text using TweetTokenizer.\n",
    "        set strip_handles = True to Twitter username handles.\n",
    "        set reduce_len = True to replace repeated character sequences of length 3 or greater with sequences of length 3.\n",
    "    \"\"\"\n",
    "    if remove_url:\n",
    "        tweet = removeURL(tweet)\n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tokens = [w.lower() for w in twtk.tokenize(str(tweet)) if w != \"\" and w is not None]\n",
    "    if remove_hashtags:\n",
    "        tokens = removeHashtags(tokens)\n",
    "    if remove_swords:\n",
    "        tokens = removeStopwords(tokens)\n",
    "    if stem_tweet:\n",
    "        tokens = stemTweet(tokens)\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data.apply(lambda x: processTweet(x['text'], remove_url=True), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hurray save mani way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would young fight age men vast major one escap war cannot fight like women children elder major refuge actual refuge econom migrant tri get europ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>illeg dump kid border like road kill refus unit hope get amnesti free educ welfar illeg countri taxpay dime scam</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>time near white state pose array problem immigr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>orban brussel european leader ignor peopl want migrant</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    text  \\\n",
       "0                                                                                                                                   hurray save mani way   \n",
       "1  would young fight age men vast major one escap war cannot fight like women children elder major refuge actual refuge econom migrant tri get europ ...   \n",
       "2                                       illeg dump kid border like road kill refus unit hope get amnesti free educ welfar illeg countri taxpay dime scam   \n",
       "3                                                                                                        time near white state pose array problem immigr   \n",
       "4                                                                                                 orban brussel european leader ignor peopl want migrant   \n",
       "\n",
       "   HS  \n",
       "0   1  \n",
       "1   1  \n",
       "2   1  \n",
       "3   0  \n",
       "4   0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    13000 non-null  object\n",
      " 1   HS      13000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 304.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'kernel_regularizer')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m tweet \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(maxlen,), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[1;32m---> 20\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(V, \u001b[39m128\u001b[39;49m, input_length\u001b[39m=\u001b[39;49mmaxlen, kernel_regularizer \u001b[39m=\u001b[39;49ml2(l\u001b[39m=\u001b[39;49ml2_coef)))\n\u001b[0;32m     21\u001b[0m model\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.2\u001b[39m))\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39madd(Bidirectional(layer\u001b[39m=\u001b[39mGRU(\u001b[39m128\u001b[39m, return_sequences\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \n\u001b[0;32m     23\u001b[0m                             kernel_regularizer \u001b[39m=\u001b[39ml2(l\u001b[39m=\u001b[39ml2_coef),\n\u001b[0;32m     24\u001b[0m                             b_regularizer\u001b[39m=\u001b[39ml2(l\u001b[39m=\u001b[39ml2_coef),\n\u001b[0;32m     25\u001b[0m                             U_regularizer\u001b[39m=\u001b[39ml2(l\u001b[39m=\u001b[39ml2_coef)),\n\u001b[0;32m     26\u001b[0m                   merge_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32m~\\Envs\\deepl\\lib\\site-packages\\keras\\layers\\embeddings.py:141\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=135'>136</a>\u001b[0m \u001b[39m# We set autocast to False, as we do not want to cast floating- point inputs\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=136'>137</a>\u001b[0m \u001b[39m# to self.dtype. In call(), we cast to int32, and casting to self.dtype\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=137'>138</a>\u001b[0m \u001b[39m# before casting to int32 might cause the int32 values to be different due\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=138'>139</a>\u001b[0m \u001b[39m# to a loss of precision.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=139'>140</a>\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mautocast\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=140'>141</a>\u001b[0m \u001b[39msuper\u001b[39m(Embedding, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=142'>143</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_dim \u001b[39m=\u001b[39m input_dim\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/layers/embeddings.py?line=143'>144</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim \u001b[39m=\u001b[39m output_dim\n",
      "File \u001b[1;32m~\\Envs\\deepl\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/tensorflow/python/training/tracking/base.py?line=626'>627</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/tensorflow/python/training/tracking/base.py?line=627'>628</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///~/Envs/deepl/lib/site-packages/tensorflow/python/training/tracking/base.py?line=628'>629</a>\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/tensorflow/python/training/tracking/base.py?line=629'>630</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/tensorflow/python/training/tracking/base.py?line=630'>631</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\Envs\\deepl\\lib\\site-packages\\keras\\engine\\base_layer.py:341\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=329'>330</a>\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=330'>331</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=331'>332</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=337'>338</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=338'>339</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=339'>340</a>\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=340'>341</a>\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=342'>343</a>\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=343'>344</a>\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=344'>345</a>\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=345'>346</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=346'>347</a>\u001b[0m         (\u001b[39misinstance\u001b[39m(trainable, (tf\u001b[39m.\u001b[39mTensor, tf\u001b[39m.\u001b[39mVariable)) \u001b[39mand\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/Envs/deepl/lib/site-packages/keras/engine/base_layer.py?line=347'>348</a>\u001b[0m          trainable\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m tf\u001b[39m.\u001b[39mbool)):\n",
      "File \u001b[1;32m~\\Envs\\deepl\\lib\\site-packages\\keras\\utils\\generic_utils.py:1174\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[0;32m   <a href='file:///~/Envs/deepl/lib/site-packages/keras/utils/generic_utils.py?line=1171'>1172</a>\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m   <a href='file:///~/Envs/deepl/lib/site-packages/keras/utils/generic_utils.py?line=1172'>1173</a>\u001b[0m   \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[1;32m-> <a href='file:///~/Envs/deepl/lib/site-packages/keras/utils/generic_utils.py?line=1173'>1174</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[1;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'kernel_regularizer')"
     ]
    }
   ],
   "source": [
    "# tweets = train_data['text']\n",
    "maxlen = 140\n",
    "data['text'] = data['text'].map(lambda x: processTweet(x))\n",
    "vocabulary_size = 30000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X_train = tokenizer.texts_to_sequences(data['text'])\n",
    "# print(sequences)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "labels = data['HS']\n",
    "Y_train = np_utils.to_categorical(labels, len(set(labels)))\n",
    "V = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "l2_coef = 0.001\n",
    "\n",
    "\n",
    "tweet = Input(shape=(maxlen,), dtype='int32')\n",
    "model = Sequential()\n",
    "model.add(Embedding(V, 128, input_length=maxlen, kernel_regularizer =l2(l=l2_coef)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(layer=GRU(128, return_sequences=False, \n",
    "                            kernel_regularizer =l2(l=l2_coef),\n",
    "                            b_regularizer=l2(l=l2_coef),\n",
    "                            U_regularizer=l2(l=l2_coef)),\n",
    "                  merge_mode='sum'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(set(labels)), kernel_regularizer =l2(l=l2_coef), activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x = Embedding(V, 128, input_length=maxlen, kernel_regularizer =l2(l=l2_coef))(tweet)\n",
    "# x = Bidirectional(layer=GRU(128, return_sequences=False, \n",
    "#                             kernel_regularizer =l2(l=l2_coef),\n",
    "#                             b_regularizer=l2(l=l2_coef),\n",
    "#                             U_regularizer=l2(l=l2_coef)),\n",
    "#                   merge_mode='sum')(x)\n",
    "# x = Dense(len(set(labels)), kernel_regularizer =l2(l=l2_coef), activation=\"softmax\")(x)\n",
    "\n",
    "# tweet2vec = Model(input=tweet, output=x)\n",
    "\n",
    "# tweet2vec.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer='RMSprop',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# tweet2vec.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "801861d9bd61459a4c0a3c69eb487aff43ff39200e70bc8535e0c2e67a3857e7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
